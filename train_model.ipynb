{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...\n",
      "conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'content-trust', 'doctor', 'repoquery', 'env')\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate img2img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8984812e77754fec9d818f63e8181df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/myid/rk42218/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "DATASET_ID=\"fusing/instructpix2pix-1000-samples\"\n",
    "#OUTPUT_DIR=\"Perman27/disaster_model\"\n",
    "OUTPUT_DIR=\"./model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/myid/rk42218/.cache/huggingface/token\n",
      "Login successful\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "11/14/2023 14:43:16 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "11/14/2023 14:43:16 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 3\n",
      "Local process index: 3\n",
      "Device: cuda:3\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'clip_sample_range', 'thresholding', 'prediction_type', 'dynamic_thresholding_ratio', 'variance_type', 'timestep_spacing', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "11/14/2023 14:43:16 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "11/14/2023 14:43:16 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 2\n",
      "Local process index: 2\n",
      "Device: cuda:2\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "{'resnet_skip_time_act', 'conv_in_kernel', 'timestep_post_act', 'conv_out_kernel', 'only_cross_attention', 'time_embedding_act_fn', 'encoder_hid_dim_type', 'use_linear_projection', 'reverse_transformer_layers_per_block', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'transformer_layers_per_block', 'time_embedding_type', 'upcast_attention', 'dropout', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'addition_embed_type_num_heads', 'resnet_time_scale_shift', 'time_embedding_dim', 'addition_embed_type', 'num_attention_heads', 'cross_attention_norm', 'num_class_embeds', 'encoder_hid_dim', 'dual_cross_attention', 'class_embed_type', 'class_embeddings_concat', 'addition_time_embed_dim', 'mid_block_type', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "11/14/2023 14:43:18 - INFO - __main__ - Initializing the InstructPix2Pix UNet from the pretrained UNet.\n",
      "Traceback (most recent call last):\n",
      "  File \"./diffusers/examples/instruct_pix2pix/train_instruct_pix2pix.py\", line 1009, in <module>\n",
      "    main()\n",
      "  File \"./diffusers/examples/instruct_pix2pix/train_instruct_pix2pix.py\", line 480, in main\n",
      "    unet.enable_xformers_memory_efficient_attention()\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 291, in enable_xformers_memory_efficient_attention\n",
      "    self.set_use_memory_efficient_attention_xformers(True, attention_op)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 255, in set_use_memory_efficient_attention_xformers\n",
      "    fn_recursive_set_mem_eff(module)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 251, in fn_recursive_set_mem_eff\n",
      "    fn_recursive_set_mem_eff(child)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 251, in fn_recursive_set_mem_eff\n",
      "    fn_recursive_set_mem_eff(child)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 251, in fn_recursive_set_mem_eff\n",
      "    fn_recursive_set_mem_eff(child)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 248, in fn_recursive_set_mem_eff\n",
      "    module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 255, in set_use_memory_efficient_attention_xformers\n",
      "    fn_recursive_set_mem_eff(module)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 251, in fn_recursive_set_mem_eff\n",
      "    fn_recursive_set_mem_eff(child)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 251, in fn_recursive_set_mem_eff\n",
      "    fn_recursive_set_mem_eff(child)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/modeling_utils.py\", line 248, in fn_recursive_set_mem_eff\n",
      "    module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/attention_processor.py\", line 268, in set_use_memory_efficient_attention_xformers\n",
      "    raise e\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/attention_processor.py\", line 263, in set_use_memory_efficient_attention_xformers\n",
      "    torch.randn((1, 2, 40), device=\"cuda\"),\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                              | 0.00/417M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|‚ñè                    | 4.19M/417M [00:00<00:23, 17.8MB/s]\u001b[A\n",
      "Downloading data:   3%|‚ñã                    | 12.6M/417M [00:00<00:10, 37.4MB/s]\u001b[A\n",
      "Downloading data:   5%|‚ñà                    | 21.0M/417M [00:00<00:08, 47.4MB/s]\u001b[A\n",
      "Downloading data:   7%|‚ñà‚ñç                   | 29.4M/417M [00:00<00:07, 53.1MB/s]\u001b[A\n",
      "Downloading data:   9%|‚ñà‚ñâ                   | 37.7M/417M [00:00<00:06, 56.4MB/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà‚ñà‚ñé                  | 46.1M/417M [00:00<00:06, 58.5MB/s]\u001b[A\n",
      "Downloading data:  13%|‚ñà‚ñà‚ñã                  | 54.5M/417M [00:01<00:06, 58.5MB/s]\u001b[A\n",
      "Downloading data:  15%|‚ñà‚ñà‚ñà‚ñè                 | 62.9M/417M [00:01<00:05, 60.3MB/s]\u001b[A\n",
      "Downloading data:  17%|‚ñà‚ñà‚ñà‚ñå                 | 71.3M/417M [00:01<00:05, 61.6MB/s]\u001b[A\n",
      "Downloading data:  19%|‚ñà‚ñà‚ñà‚ñà                 | 79.7M/417M [00:01<00:05, 62.0MB/s]\u001b[A\n",
      "Downloading data:  21%|‚ñà‚ñà‚ñà‚ñà‚ñç                | 88.1M/417M [00:01<00:05, 62.5MB/s]\u001b[A\n",
      "Downloading data:  23%|‚ñà‚ñà‚ñà‚ñà‚ñä                | 96.5M/417M [00:01<00:06, 51.1MB/s]\u001b[A\n",
      "Downloading data:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 105M/417M [00:01<00:05, 54.1MB/s]\u001b[A\n",
      "Downloading data:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 113M/417M [00:02<00:11, 27.2MB/s]\u001b[A[2023-11-14 14:43:22,058] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57385 closing signal SIGTERM\n",
      "[2023-11-14 14:43:22,059] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57386 closing signal SIGTERM\n",
      "[2023-11-14 14:43:22,059] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57387 closing signal SIGTERM\n",
      "[2023-11-14 14:43:22,889] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 57384) of binary: /home/myid/rk42218/miniconda3/envs/img2img/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 985, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 654, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "./diffusers/examples/instruct_pix2pix/train_instruct_pix2pix.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-11-14_14:43:22\n",
      "  host      : csci-cscuda\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 57384)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "login(token=\"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\")\n",
    "add_to_git_credential=True\n",
    "\n",
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "DATASET_ID=\"fusing/instructpix2pix-1000-samples\"\n",
    "#OUTPUT_DIR=\"Perman27/disaster_model\"\n",
    "OUTPUT_DIR=\"./model\"\n",
    "\n",
    "\n",
    "!accelerate launch --mixed_precision=\"fp16\" ./diffusers/examples/instruct_pix2pix/train_instruct_pix2pix.py \\\n",
    "    --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "    --dataset_name=$DATASET_ID \\\n",
    "    --enable_xformers_memory_efficient_attention \\\n",
    "    --resolution=256 --random_flip \\\n",
    "    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n",
    "    --max_train_steps=15000 \\\n",
    "    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n",
    "    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n",
    "    --conditioning_dropout_prob=0.05 \\\n",
    "    --mixed_precision=fp16 \\\n",
    "    --seed=42 \\\n",
    "    --output_dir=$OUTPUT_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7227d36845d043449018c7f77c873be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n",
    "pipeline.unet.config[\"in_channels\"]\n",
    "pipe.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "11/14/2023 14:46:29 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 2\n",
      "Local process index: 2\n",
      "Device: cuda:2\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "11/14/2023 14:46:29 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "11/14/2023 14:46:29 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 3\n",
      "Local process index: 3\n",
      "Device: cuda:3\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "11/14/2023 14:46:29 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'clip_sample_range', 'variance_type', 'dynamic_thresholding_ratio', 'prediction_type', 'sample_max_value', 'timestep_spacing', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "{'conv_out_kernel', 'upcast_attention', 'resnet_skip_time_act', 'addition_time_embed_dim', 'timestep_post_act', 'time_embedding_dim', 'resnet_out_scale_factor', 'num_attention_heads', 'only_cross_attention', 'attention_type', 'dropout', 'encoder_hid_dim', 'mid_block_type', 'num_class_embeds', 'class_embed_type', 'time_cond_proj_dim', 'reverse_transformer_layers_per_block', 'encoder_hid_dim_type', 'class_embeddings_concat', 'addition_embed_type_num_heads', 'projection_class_embeddings_input_dim', 'use_linear_projection', 'time_embedding_act_fn', 'transformer_layers_per_block', 'addition_embed_type', 'cross_attention_norm', 'time_embedding_type', 'resnet_time_scale_shift', 'conv_in_kernel', 'mid_block_only_cross_attention', 'dual_cross_attention'} was not found in config. Values will be initialized to default values.\n",
      "Traceback (most recent call last):\n",
      "  File \"./diffusers/examples/text_to_image/train_text_to_image_lora.py\", line 927, in <module>\n",
      "    main()\n",
      "  File \"./diffusers/examples/text_to_image/train_text_to_image_lora.py\", line 443, in main\n",
      "    unet.to(accelerator.device, dtype=weight_dtype)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.80k/1.80k [00:00<00:00, 11.7MB/s]\n",
      "Downloading metadata: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 731/731 [00:00<00:00, 5.29MB/s]\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/99.7M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   4%|‚ñä                   | 4.19M/99.7M [00:00<00:12, 7.37MB/s]\u001b[A\n",
      "Downloading data:  13%|‚ñà‚ñà‚ñå                 | 12.6M/99.7M [00:00<00:04, 19.7MB/s]\u001b[A[2023-11-14 14:46:35,593] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57488 closing signal SIGTERM\n",
      "[2023-11-14 14:46:35,593] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57489 closing signal SIGTERM\n",
      "[2023-11-14 14:46:35,594] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57490 closing signal SIGTERM\n",
      "[2023-11-14 14:46:35,872] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 57487) of binary: /home/myid/rk42218/miniconda3/envs/img2img/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 985, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 654, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "./diffusers/examples/text_to_image/train_text_to_image_lora.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-11-14_14:46:35\n",
      "  host      : csci-cscuda\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 57487)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n",
    "OUTPUT_DIR=\"./models/lora/pokemon\"\n",
    "\n",
    "!accelerate launch --mixed_precision=\"fp16\"  ./diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --dataset_name=$DATASET_NAME \\\n",
    "  --dataloader_num_workers=8 \\\n",
    "  --resolution=512 --center_crop --random_flip \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=15000 \\\n",
    "  --learning_rate=1e-04 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "  --output_dir=${OUTPUT_DIR} \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --validation_prompt=\"Totoro\" \\\n",
    "  --seed=42 \\\n",
    "  --report_to=wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "DATA_DIR=\"./finaldata/small/\"\n",
    "\n",
    "!accelerate launch ./textual_inversion.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --train_data_dir=$DATA_DIR \\\n",
    "  --learnable_property=\"object\" \\\n",
    "  --placeholder_token=\"<cat-toy>\" --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=30 \\\n",
    "  --learning_rate=5.0e-04 --scale_lr \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --output_dir=\"./model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCHELASTIC_ERROR_FILE=\"./error_file.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "11/14/2023 23:11:31 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "11/14/2023 23:11:32 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "11/14/2023 23:11:32 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 2\n",
      "Local process index: 2\n",
      "Device: cuda:2\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "11/14/2023 23:11:32 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 3\n",
      "Local process index: 3\n",
      "Device: cuda:3\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "{'thresholding', 'variance_type', 'prediction_type', 'timestep_spacing', 'dynamic_thresholding_ratio', 'sample_max_value', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'force_upcast', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "{'addition_time_embed_dim', 'attention_type', 'encoder_hid_dim', 'only_cross_attention', 'addition_embed_type_num_heads', 'class_embed_type', 'resnet_skip_time_act', 'time_embedding_act_fn', 'cross_attention_norm', 'time_cond_proj_dim', 'conv_in_kernel', 'time_embedding_dim', 'addition_embed_type', 'transformer_layers_per_block', 'conv_out_kernel', 'class_embeddings_concat', 'use_linear_projection', 'timestep_post_act', 'reverse_transformer_layers_per_block', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'dropout', 'upcast_attention', 'encoder_hid_dim_type', 'num_attention_heads', 'projection_class_embeddings_input_dim', 'dual_cross_attention', 'time_embedding_type', 'mid_block_type', 'num_class_embeds', 'resnet_time_scale_shift'} was not found in config. Values will be initialized to default values.\n",
      "Traceback (most recent call last):\n",
      "  File \"./edited_code.py\", line 969, in <module>\n",
      "    main()\n",
      "  File \"./edited_code.py\", line 694, in main\n",
      "    train_dataset = DisasterImageDataset(\n",
      "TypeError: __init__() got an unexpected keyword argument 'placeholder_token'\n",
      "Traceback (most recent call last):\n",
      "  File \"./edited_code.py\", line 969, in <module>\n",
      "    main()\n",
      "  File \"./edited_code.py\", line 694, in main\n",
      "    train_dataset = DisasterImageDataset(\n",
      "TypeError: __init__() got an unexpected keyword argument 'placeholder_token'\n",
      "Traceback (most recent call last):\n",
      "  File \"./edited_code.py\", line 969, in <module>\n",
      "    main()\n",
      "  File \"./edited_code.py\", line 694, in main\n",
      "    train_dataset = DisasterImageDataset(\n",
      "TypeError: __init__() got an unexpected keyword argument 'placeholder_token'\n",
      "Traceback (most recent call last):\n",
      "  File \"./edited_code.py\", line 969, in <module>\n",
      "    main()\n",
      "  File \"./edited_code.py\", line 694, in main\n",
      "    train_dataset = DisasterImageDataset(\n",
      "TypeError: __init__() got an unexpected keyword argument 'placeholder_token'\n",
      "[2023-11-14 23:11:38,088] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1918038) of binary: /home/myid/rk42218/miniconda3/envs/img2img/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 985, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 654, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "./edited_code.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2023-11-14_23:11:38\n",
      "  host      : csci-cscuda\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 1918039)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2023-11-14_23:11:38\n",
      "  host      : csci-cscuda\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 1918040)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[3]:\n",
      "  time      : 2023-11-14_23:11:38\n",
      "  host      : csci-cscuda\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 1918041)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-11-14_23:11:38\n",
      "  host      : csci-cscuda\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1918038)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "PRE_DATA_DIR=\"./finaldata/test/pre/\"\n",
    "POST_DATA_DIR=\"./finaldata/test/post/\"\n",
    "PRE_CSV_FILE=\"./finaldata/test/test_pre_prompts.csv\"\n",
    "POST_CSV_FILE=\"./finaldata/test/test_post_prompts.csv\"\n",
    "TORCHELASTIC_ERROR_FILE=\"./error_file.txt\"\n",
    "\n",
    "!accelerate launch ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3000 \\\n",
    "  --learning_rate=2.0e-04 \\\n",
    "  --output_dir=\"./model/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export PRE_DATA_DIR=\"./finaldata/hurricane_pre/\"\n",
    "export POST_DATA_DIR=\"./finaldata/hurricane_post/\"\n",
    "export PRE_CSV_FILE=\"./finaldata/pre_hurricane_prompts.csv\"\n",
    "export POST_CSV_FILE=\"./finaldata/post_hurricane_prompts.csv\"\n",
    "export CUDA_VISIBLE_DEVICES=1,2,3\n",
    "\n",
    "accelerate launch --num_processes=3 ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3000 \\\n",
    "  --learning_rate=2.0e-04 \\\n",
    "  --output_dir=\"./model_local_save/model_hurricane3000_demo/\" \\\n",
    "  --save_as_full_pipeline \\\n",
    "  --report_to=wandb \\\n",
    "  --validation_prompt \"Post Disaster Satellite Images of an Hurricane effected area.\" \\\n",
    "  --num_validation_images 4 \\\n",
    "  --validation_steps 3000 \\\n",
    "  --seed 42 \\\n",
    "  --checkpointing_steps 500 \\\n",
    "  --logging_dir \"./logs\" \\\n",
    "  --dataloader_num_workers 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export PRE_DATA_DIR=\"./finaldata/hurricane_pre/\"\n",
    "export POST_DATA_DIR=\"./finaldata/hurricane_post/\"\n",
    "export PRE_CSV_FILE=\"./finaldata/pre_hurricane_prompts.csv\"\n",
    "export POST_CSV_FILE=\"./finaldata/post_hurricane_prompts.csv\"\n",
    "export CUDA_VISIBLE_DEVICES=1,2,3\n",
    "\n",
    "\n",
    "accelerate launch --num_processes=3 ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3 \\\n",
    "  --learning_rate=2.0e-04 \\\n",
    "  --output_dir=\"./model_local_save/model_hurricane3000/\" \\\n",
    "  --save_as_full_pipeline \\\n",
    "  --report_to=wandb \\\n",
    "  --validation_prompt \"Post Disaster Satellite Images of an Hurricane effected area.\" \\\n",
    "  --num_validation_images 4 \\\n",
    "  --validation_steps  100\n",
    "\n",
    "  \n",
    "  \\\n",
    "  --resume_from_checkpoint=\"checkpoint-1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export PRE_DATA_DIR=\"./finaldata/hurricane_pre/\"\n",
    "export POST_DATA_DIR=\"./finaldata/hurricane_post/\"\n",
    "export PRE_CSV_FILE=\"./finaldata/pre_hurricane_prompts.csv\"\n",
    "export POST_CSV_FILE=\"./finaldata/post_hurricane_prompts.csv\"\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "\n",
    "accelerate launch --num_processes=3 ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=6000 \\\n",
    "  --learning_rate=2.0e-04 \\\n",
    "  --output_dir=\"./model_hurricane6000/\" \n",
    "  \n",
    "  \\\n",
    "  --resume_from_checkpoint=\"checkpoint-1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mruthikkale1\u001b[0m (\u001b[33mperman\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mruthikkale1\u001b[0m (\u001b[33mperman\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mruthikkale1\u001b[0m (\u001b[33mperman\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./model_hurricane/wandb/run-20231118_184925-bz4huc3c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./model_hurricane/wandb/run-20231118_184925-pq82knc6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgallant-surf-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/perman/your_project_name\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/perman/your_project_name/runs/bz4huc3c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdashing-dragon-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/perman/your_project_name\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/perman/your_project_name/runs/pq82knc6\u001b[0m\n",
      "11/18/2023 18:49:27 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 3\n",
      "Process index: 2\n",
      "Local process index: 2\n",
      "Device: cuda:2\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "11/18/2023 18:49:27 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 3\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./model_hurricane/wandb/run-20231118_184925-6qiuoi07\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33musual-sponge-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/perman/your_project_name\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/perman/your_project_name/runs/6qiuoi07\u001b[0m\n",
      "11/18/2023 18:49:27 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 3\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "{'prediction_type', 'timestep_spacing', 'thresholding', 'dynamic_thresholding_ratio', 'clip_sample_range', 'sample_max_value', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "{'num_attention_heads', 'timestep_post_act', 'cross_attention_norm', 'conv_in_kernel', 'addition_embed_type', 'transformer_layers_per_block', 'encoder_hid_dim_type', 'attention_type', 'time_embedding_type', 'resnet_time_scale_shift', 'class_embed_type', 'mid_block_type', 'dropout', 'conv_out_kernel', 'time_cond_proj_dim', 'use_linear_projection', 'time_embedding_act_fn', 'mid_block_only_cross_attention', 'addition_embed_type_num_heads', 'only_cross_attention', 'addition_time_embed_dim', 'resnet_skip_time_act', 'num_class_embeds', 'projection_class_embeddings_input_dim', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'dual_cross_attention', 'upcast_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'class_embeddings_concat'} was not found in config. Values will be initialized to default values.\n",
      "Batch size: 3\n",
      "DataLoader Num Workers: 0\n",
      "Batch size: 3\n",
      "DataLoader Num Workers: 0\n",
      "Batch size: 3\n",
      "DataLoader Num Workers: 0\n",
      "11/18/2023 18:49:32 - INFO - __main__ - ***** Running training *****\n",
      "11/18/2023 18:49:32 - INFO - __main__ -   Num examples = 1266\n",
      "11/18/2023 18:49:32 - INFO - __main__ -   Num Epochs = 29\n",
      "11/18/2023 18:49:32 - INFO - __main__ -   Instantaneous batch size per device = 3\n",
      "11/18/2023 18:49:32 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 36\n",
      "11/18/2023 18:49:32 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "11/18/2023 18:49:32 - INFO - __main__ -   Total optimization steps = 3000\n",
      "Steps:   0%|                                           | 0/3000 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"./edited_code.py\", line 1041, in <module>\n",
      "    main()\n",
      "  File \"./edited_code.py\", line 870, in main\n",
      "    pre_latents = vae.encode(batch[\"pre_pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample().detach()\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/autoencoder_kl.py\", line 274, in encode\n",
      "    h = self.encoder(x)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/vae.py\", line 165, in forward\n",
      "    sample = down_block(sample)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py\", line 1323, in forward\n",
      "    hidden_states = resnet(hidden_states, temb=None, scale=scale)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/models/resnet.py\", line 755, in forward\n",
      "    output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 10.75 GiB of which 21.31 MiB is free. Process 3113078 has 4.40 GiB memory in use. Process 3629974 has 1.14 GiB memory in use. Including non-PyTorch memory, this process has 5.13 GiB memory in use. Of the allocated memory 4.78 GiB is allocated by PyTorch, and 75.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdashing-dragon-13\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/perman/your_project_name/runs/pq82knc6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./model_hurricane/wandb/run-20231118_184925-pq82knc6/logs\u001b[0m\n",
      "[2023-11-18 18:49:40,526] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3691365 closing signal SIGTERM\n",
      "[2023-11-18 18:49:40,526] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3691366 closing signal SIGTERM\n",
      "[2023-11-18 18:49:40,740] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3691364) of binary: /home/myid/rk42218/miniconda3/envs/img2img/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 985, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 654, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/myid/rk42218/miniconda3/envs/img2img/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "./edited_code.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-11-18_18:49:40\n",
      "  host      : csci-cscuda\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3691364)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "PRE_DATA_DIR=\"./finaldata/hurricane_pre/\"\n",
    "POST_DATA_DIR=\"./finaldata/hurricane_post/\"\n",
    "PRE_CSV_FILE=\"./finaldata/pre_hurricane_prompts.csv\"\n",
    "POST_CSV_FILE=\"./finaldata/post_hurricane_prompts.csv\"\n",
    "CUDA_VISIBLE_DEVICES=1,2,3\n",
    "\n",
    "\n",
    "!accelerate launch --num_processes=3 ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3000 \\\n",
    "  --learning_rate=2.0e-04 \\\n",
    "  --output_dir=\"./model_hurricane/\" \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "accelerate launch --num_processes=3 ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=6000 \\\n",
    "  --learning_rate=1.0e-04 \\\n",
    "  --output_dir=\"./model_variant2/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export PRE_DATA_DIR=\"./finaldata/hurricane_pre/\"\n",
    "export POST_DATA_DIR=\"./finaldata/hurricane_post/\"\n",
    "export PRE_CSV_FILE=\"./finaldata/pre_hurricane_prompts.csv\"\n",
    "export POST_CSV_FILE=\"./finaldata/post_hurricane_prompts.csv\"\n",
    "export CUDA_VISIBLE_DEVICES=1,2,3\n",
    "\n",
    "\n",
    "accelerate launch --num_processes=3 ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3 \\\n",
    "  --learning_rate=5.0e-04 \\\n",
    "  --output_dir=\"./model_hurricane_5variant3/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusers version: 0.23.1\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "\n",
    "print(\"Diffusers version:\", diffusers.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "print(diffusers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named model_index.json found in directory ./model_hurricane3000/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/myid/rk42218/train_model.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     generated_image\u001b[39m.\u001b[39mshow()  \u001b[39m# Display the image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     main()\n",
      "\u001b[1;32m/home/myid/rk42218/train_model.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m CLIPTokenizer\u001b[39m.\u001b[39mfrom_pretrained(output_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Load the fine-tuned Stable Diffusion pipeline\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m pipeline \u001b[39m=\u001b[39m StableDiffusionPipeline\u001b[39m.\u001b[39;49mfrom_pretrained(output_dir, tokenizer\u001b[39m=\u001b[39;49mtokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Move pipeline to GPU if available\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m pipeline \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/pipelines/pipeline_utils.py:1110\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1108\u001b[0m     cached_folder \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m-> 1110\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_config(cached_folder)\n\u001b[1;32m   1112\u001b[0m \u001b[39m# pop out \"_ignore_files\" as it is only needed for download\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m config_dict\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_ignore_files\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/img2img/lib/python3.8/site-packages/diffusers/configuration_utils.py:364\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         config_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pretrained_model_name_or_path, subfolder, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_name)\n\u001b[1;32m    363\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    365\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError no file named \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_name\u001b[39m}\u001b[39;00m\u001b[39m found in directory \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m         )\n\u001b[1;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m         \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named model_index.json found in directory ./model_hurricane3000/."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image from a file path and convert it to a NumPy array.\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "def main():\n",
    "    output_dir = \"./model_hurricane3000/\"  # Replace with the path to your model directory\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "    # Load the fine-tuned Stable Diffusion pipeline\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(output_dir, tokenizer=tokenizer)\n",
    "\n",
    "    # Move pipeline to GPU if available\n",
    "    pipeline = pipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Prepare your pre-disaster satellite image and the corresponding text prompt\n",
    "    pre_disaster_image_path = \"./finaldata/test/pre/hurricane-florence_00000003_pre_disaster_left.png\"  # Replace with your image path\n",
    "    pre_disaster_image = load_image(pre_disaster_image_path)\n",
    "\n",
    "    text_prompt = \"Post Disaster Satellite Image of a Hurricane\"  # Replace with your text prompt\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate post-disaster image\n",
    "        generated_image = pipeline(pre_disaster_image, text_prompt).images[0]\n",
    "\n",
    "    # Save or display the generated image\n",
    "    generated_image.save(\"generated_post_disaster_image.jpg\")  # Save the generated image\n",
    "    generated_image.show()  # Display the image\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in learned_embeds_dict: dict_keys(['<disaster>'])\n",
      "Loaded embeddings shape: torch.Size([1, 768])\n",
      "Loaded embeddings type: torch.float32\n",
      "Current CUDA device: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9011b1431bdf440f93d4cd845b437930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to allocate the model on GPU, switching to CPU. Error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during image generation: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPTokenizer\n",
    "from PIL import Image\n",
    "import safetensors.torch as st\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "\n",
    "# Function to load embeddings\n",
    "def load_embeddings(embeddings_path):\n",
    "    with open(embeddings_path, 'rb') as f:\n",
    "        embeddings_data = f.read()\n",
    "    learned_embeds_dict = st.load(embeddings_data)\n",
    "    return learned_embeds_dict['<disaster>']\n",
    "\n",
    "# Function to generate image\n",
    "def generate_image(pipe, prompt, pre_disaster_image_path):\n",
    "    pre_disaster_image = Image.open(pre_disaster_image_path)\n",
    "    # TODO: Preprocess the pre-disaster image\n",
    "    # ...\n",
    "\n",
    "    # Generate the post-disaster image\n",
    "    generated_image = pipe(prompt).images[0]\n",
    "    return generated_image\n",
    "\n",
    "# Load tokenizer and embeddings\n",
    "tokenizer_path = './model_hurricane3000/'  # Update with your path\n",
    "embeddings_path = './model_hurricane3000/learned_embeds-steps-3000.safetensors'  # Update with your path\n",
    "tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path)\n",
    "# Debugging: Print keys in the embeddings dictionary\n",
    "print(\"Keys in learned_embeds_dict:\", learned_embeds_dict.keys())\n",
    "\n",
    "# ... [Previous setup code]\n",
    "\n",
    "# Load embeddings and check compatibility\n",
    "learned_embeds = load_embeddings(embeddings_path)\n",
    "print(\"Loaded embeddings shape:\", learned_embeds.shape)\n",
    "print(\"Loaded embeddings type:\", learned_embeds.dtype)\n",
    "\n",
    "# Check current device\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current CUDA device:\", current_device)\n",
    "\n",
    "# Initialize model and pipeline with error handling\n",
    "try:\n",
    "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "    pipe.text_encoder.get_input_embeddings().weight.data = learned_embeds\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Failed to allocate the model on GPU, switching to CPU. Error: {e}\")\n",
    "    pipe = pipe.to(\"cpu\")\n",
    "\n",
    "# Generate image with error handling\n",
    "try:\n",
    "    prompt = \"Post Disaster Satellite Image of a Hurricane affected area.\"\n",
    "    pre_disaster_image_path = \"./finaldata/test/pre/hurricane-florence_00000003_pre_disaster_left.png\"\n",
    "    generated_image = generate_image(pipe, prompt, pre_disaster_image_path)\n",
    "    generated_image.save(\"generated_post_disaster_image.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during image generation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL, StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "model_path = './model_hurricane3000/'  # Main directory path\n",
    "checkpoint_path = model_path + 'checkpoint-3000/'  # Adjust to the specific checkpoint you want to use\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_path)\n",
    "\n",
    "from transformers import CLIPTextConfig, CLIPTextModel\n",
    "\n",
    "# Load the configuration manually\n",
    "config_path = checkpoint_path + 'config.json'\n",
    "config = CLIPTextConfig.from_json_file(config_path)\n",
    "\n",
    "# Create the CLIPTextModel with the loaded configuration\n",
    "text_encoder = CLIPTextModel(config)\n",
    "\n",
    "# Load each component of the model using the checkpoint\n",
    "# Ensure that 'config.json' is present in 'checkpoint_path' and contains the necessary configuration\n",
    "vae = AutoencoderKL.from_pretrained(checkpoint_path)\n",
    "unet = UNet2DConditionModel.from_pretrained(checkpoint_path)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(checkpoint_path)\n",
    "\n",
    "\n",
    "# Reconstruct the pipeline\n",
    "pipeline = StableDiffusionPipeline(\n",
    "    text_encoder=text_encoder,\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    "    tokenizer=tokenizer,\n",
    "    noise_scheduler=noise_scheduler\n",
    ")\n",
    "\n",
    "# Resize token embeddings and initialize custom embeddings\n",
    "initializer_token_id = tokenizer.convert_tokens_to_ids(initializer_token)\n",
    "placeholder_token_ids = tokenizer.convert_tokens_to_ids([placeholder_token])\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "with torch.no_grad():\n",
    "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "    for token_id in placeholder_token_ids:\n",
    "        token_embeds[token_id] = token_embeds[initializer_token_id].clone()\n",
    "\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_preprocess_image(image_path, size=(512, 512)):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize(size)\n",
    "    return np.array(image)\n",
    "\n",
    "# Function to generate a post-disaster image based on a text prompt and a pre-disaster image\n",
    "def generate_image(pipeline, pre_image_path, text_prompt):\n",
    "    # Load and preprocess the pre-disaster image\n",
    "    pre_image = load_preprocess_image(pre_image_path)\n",
    "\n",
    "    # Encode the text prompt\n",
    "    inputs = tokenizer(text_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate a post-disaster image\n",
    "    with torch.no_grad():\n",
    "        # Note: The pipeline function call needs to be adjusted according to how your model handles image and text inputs\n",
    "        generated_image = pipeline(image=pre_image, text_input=inputs[\"input_ids\"], height=512, width=512).images[0]\n",
    "\n",
    "    return generated_image\n",
    "\n",
    "# Example usage\n",
    "pre_disaster_image_path = \"./finaldata/test/pre/hurricane-florence_00000003_pre_disaster_left.png\"  # Path to the pre-disaster image\n",
    "text_prompt = \"Post Disaster Satellite Image of a Hurricane affected area.\"\n",
    "post_disaster_image = generate_image(pipeline, pre_disaster_image_path, text_prompt)\n",
    "\n",
    "# Save or display the generated image\n",
    "post_disaster_image_path = './post_disaster_image.jpg'\n",
    "post_disaster_image.save(post_disaster_image_path)\n",
    "post_disaster_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the embeddings dictionary: dict_keys(['<disaster>'])\n"
     ]
    }
   ],
   "source": [
    "import safetensors.torch as st\n",
    "\n",
    "embeddings_path = './model_hurricane3000/learned_embeds-steps-3000.safetensors'\n",
    "\n",
    "# Read the file as bytes\n",
    "with open(embeddings_path, 'rb') as f:\n",
    "    embeddings_data = f.read()\n",
    "\n",
    "# Load the embeddings using safetensors\n",
    "learned_embeds_dict = st.load(embeddings_data)\n",
    "\n",
    "# Print out the keys in the dictionary\n",
    "print(\"Keys in the embeddings dictionary:\", learned_embeds_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0, Name: NVIDIA GeForce RTX 2080 Ti\n",
      "\tMemory Total: 11264.0 MB\n",
      "\tMemory Free: 10955.0 MB\n",
      "\tMemory Used: 55.0 MB\n",
      "\tMemory Utilization: 0.48828125%\n",
      "\n",
      "GPU: 1, Name: NVIDIA GeForce RTX 2080 Ti\n",
      "\tMemory Total: 11264.0 MB\n",
      "\tMemory Free: 11010.0 MB\n",
      "\tMemory Used: 1.0 MB\n",
      "\tMemory Utilization: 0.00887784090909091%\n",
      "\n",
      "GPU: 2, Name: NVIDIA GeForce RTX 2080 Ti\n",
      "\tMemory Total: 11264.0 MB\n",
      "\tMemory Free: 11010.0 MB\n",
      "\tMemory Used: 1.0 MB\n",
      "\tMemory Utilization: 0.00887784090909091%\n",
      "\n",
      "GPU: 3, Name: NVIDIA GeForce RTX 2080 Ti\n",
      "\tMemory Total: 11264.0 MB\n",
      "\tMemory Free: 11010.0 MB\n",
      "\tMemory Used: 1.0 MB\n",
      "\tMemory Utilization: 0.00887784090909091%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "\n",
    "# Get the list of all available GPU objects\n",
    "gpus = GPUtil.getGPUs()\n",
    "\n",
    "# Loop through all GPUs and print their memory usage\n",
    "for gpu in gpus:\n",
    "    print(f\"GPU: {gpu.id}, Name: {gpu.name}\")\n",
    "    print(f\"\\tMemory Total: {gpu.memoryTotal} MB\")\n",
    "    print(f\"\\tMemory Free: {gpu.memoryFree} MB\")\n",
    "    print(f\"\\tMemory Used: {gpu.memoryUsed} MB\")\n",
    "    print(f\"\\tMemory Utilization: {gpu.memoryUtil*100}%\")\n",
    "    print(\"\")\n",
    "\n",
    "# Now you can decide which GPUs to use based on their memory availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "    special_tokens_map.json\n",
      "    training_parameters.json\n",
      "    diffusion_pytorch_model.safetensors\n",
      "    merges.txt\n",
      "    added_tokens.json\n",
      "    learned_embeds.safetensors\n",
      "    vocab.json\n",
      "    training_args.json\n",
      "    tokenizer_config.json\n",
      "    model.safetensors\n",
      "    environment_info.json\n",
      "    config.json\n",
      "wandb/\n",
      "    debug-internal.log\n",
      "    debug.log\n",
      "    run-20231122_184350-q6fuprv7/\n",
      "        run-q6fuprv7.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184725-79nfvs3w/\n",
      "        run-79nfvs3w.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184725-nvc08vtc/\n",
      "        run-nvc08vtc.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184220-3jcsivdl/\n",
      "        run-3jcsivdl.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184220-uvf11od1/\n",
      "        run-uvf11od1.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184350-1jw2hoxv/\n",
      "        run-1jw2hoxv.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184220-kjmrjcyb/\n",
      "        run-kjmrjcyb.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184725-q6dblxkv/\n",
      "        run-q6dblxkv.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "    run-20231122_184350-aazvzp71/\n",
      "        run-aazvzp71.wandb\n",
      "        files/\n",
      "            config.yaml\n",
      "            wandb-summary.json\n",
      "            output.log\n",
      "            requirements.txt\n",
      "            wandb-metadata.json\n",
      "            conda-environment.yaml\n",
      "        tmp/\n",
      "            code/\n",
      "        logs/\n",
      "            debug-internal.log\n",
      "            debug.log\n",
      "logs/\n",
      "    textual_inversion/\n",
      "        events.out.tfevents.1700696546.csci-cscuda.692131.0\n",
      "        events.out.tfevents.1700696013.csci-cscuda.687447.0\n",
      "        events.out.tfevents.1700696636.csci-cscuda.693146.0\n",
      "        events.out.tfevents.1700696851.csci-cscuda.695184.0\n",
      "        1700696013.7093484/\n",
      "            hparams.yml\n",
      "        1700696636.5466027/\n",
      "            events.out.tfevents.1700696636.csci-cscuda.693146.1\n",
      "        1700696851.8097427/\n",
      "            hparams.yml\n",
      "        1700696546.6372206/\n",
      "            hparams.yml\n",
      "        1700696851.8079505/\n",
      "            events.out.tfevents.1700696851.csci-cscuda.695184.1\n",
      "        1700696636.5485086/\n",
      "            hparams.yml\n",
      "        1700696546.6354864/\n",
      "            events.out.tfevents.1700696546.csci-cscuda.692131.1\n",
      "        1700696013.7078385/\n",
      "            events.out.tfevents.1700696013.csci-cscuda.687447.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_dir_structure(start_path):\n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        level = root.replace(start_path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{sub_indent}{f}')\n",
    "\n",
    "# Replace 'your_directory_path' with the path of the directory you want to examine\n",
    "directory_path = './model_local_save/model_hurricane3000'\n",
    "print_dir_structure(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Device ID': 0, 'Name': 'NVIDIA GeForce RTX 2080 Ti', 'Total Memory (GB)': 10.75, 'Multiprocessors': 68, 'Compute Capability': '7.5'}, {'Device ID': 1, 'Name': 'NVIDIA GeForce RTX 2080 Ti', 'Total Memory (GB)': 10.75, 'Multiprocessors': 68, 'Compute Capability': '7.5'}, {'Device ID': 2, 'Name': 'NVIDIA GeForce RTX 2080 Ti', 'Total Memory (GB)': 10.75, 'Multiprocessors': 68, 'Compute Capability': '7.5'}, {'Device ID': 3, 'Name': 'NVIDIA GeForce RTX 2080 Ti', 'Total Memory (GB)': 10.75, 'Multiprocessors': 68, 'Compute Capability': '7.5'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to get the number of available GPUs and their details\n",
    "def get_gpu_details():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    gpu_details = []\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        gpu_info = {\n",
    "            \"Device ID\": i,\n",
    "            \"Name\": gpu.name,\n",
    "            \"Total Memory (GB)\": round(gpu.total_memory / (1024 ** 3), 2),\n",
    "            \"Multiprocessors\": gpu.multi_processor_count,\n",
    "            \"Compute Capability\": f\"{gpu.major}.{gpu.minor}\"\n",
    "        }\n",
    "        gpu_details.append(gpu_info)\n",
    "\n",
    "    return gpu_details\n",
    "\n",
    "# Getting GPU details\n",
    "gpu_details = get_gpu_details()\n",
    "print(gpu_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./finaldata/val_post_hurricane_prompts.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the directory path where images are stored\n",
    "image_dir = \"./finaldata/val_hurricane_post/\"\n",
    "\n",
    "# Define the path for the CSV file you want to create\n",
    "csv_file_path = \"./finaldata/val_post_hurricane_prompts.csv\"\n",
    "\n",
    "# List all files in the directory\n",
    "file_list = os.listdir(image_dir)\n",
    "\n",
    "# Process the files and write to CSV\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['File Name', 'Description'])  # Write the header row\n",
    "\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".png\"):  # Check if the file is a PNG image\n",
    "            # Extract the base name (without _post_disaster_left.png)\n",
    "            base_name = filename.split('_')[0]\n",
    "            # Replace dashes with spaces and capitalize each word for the description\n",
    "            description = 'Pre Disaster Satellite Image of ' + base_name.replace('-', ' ').title()\n",
    "            # Write the original file name and the description to the CSV\n",
    "            writer.writerow([filename, description])\n",
    "\n",
    "# Output the path to the CSV file\n",
    "csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./finaldata/val_pre_hurricane_prompts.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the directory path where images are stored\n",
    "image_dir = \"./finaldata/val_hurricane_pre/\"\n",
    "\n",
    "# Define the path for the CSV file you want to create\n",
    "csv_file_path = \"./finaldata/val_pre_hurricane_prompts.csv\"\n",
    "\n",
    "# List all files in the directory\n",
    "file_list = os.listdir(image_dir)\n",
    "\n",
    "# Process the files and write to CSV\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['File Name', 'Description'])  # Write the header row\n",
    "\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".png\"):  # Check if the file is a PNG image\n",
    "            # Extract the base name (without _post_disaster_left.png)\n",
    "            base_name = filename.split('_')[0]\n",
    "            # Replace dashes with spaces and capitalize each word for the description\n",
    "            description = 'Pre Disaster Satellite Image of ' + base_name.replace('-', ' ').title()\n",
    "            # Write the original file name and the description to the CSV\n",
    "            writer.writerow([filename, description])\n",
    "\n",
    "# Output the path to the CSV file\n",
    "csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mruthikkale1\u001b[0m (\u001b[33mperman\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/myid/rk42218/.netrc\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/myid/rk42218/train_model.ipynb Cell 58\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# Generate the post-disaster image using the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     generated_image_tensor \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m         image_tensor\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),  \u001b[39m# Add batch dimension\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m         text_prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m         num_inference_steps\u001b[39m=\u001b[39;49mnum_inference_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         guidance_scale\u001b[39m=\u001b[39;49mguidance_scale\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     )\u001b[39m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m# Convert the generated tensor to an image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m generated_image \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToPILImage()(generated_image_tensor\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))  \u001b[39m# Remove batch dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/edited_code.py:143\u001b[0m, in \u001b[0;36mCustomStableDiffusionPipeline.__call__\u001b[0;34m(self, image, text_prompt, num_inference_steps, temperature, guidance_scale)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, image, text_prompt, num_inference_steps\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, guidance_scale\u001b[39m=\u001b[39m\u001b[39m7.5\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     image_tensor, input_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(image, text_prompt)\n\u001b[1;32m    145\u001b[0m     \u001b[39m# Adjust the inference process with new parameters\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     post_disaster_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_post_disaster_image(\n\u001b[1;32m    147\u001b[0m         image_tensor, \n\u001b[1;32m    148\u001b[0m         input_ids, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m         guidance_scale\u001b[39m=\u001b[39mguidance_scale\n\u001b[1;32m    152\u001b[0m     )\n",
      "File \u001b[0;32m~/edited_code.py:129\u001b[0m, in \u001b[0;36mCustomStableDiffusionPipeline.preprocess\u001b[0;34m(self, image, input_ids)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(\u001b[39mself\u001b[39m, image, input_ids):\n\u001b[1;32m    128\u001b[0m     image_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_to_tensor(image)\n\u001b[0;32m--> 129\u001b[0m     input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# Add batch dimension\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m image_tensor, input_ids\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from edited_code import CustomStableDiffusionPipeline\n",
    "\n",
    "def load_model(model_dir, checkpoint_name=\"checkpoint-3000\"):\n",
    "    # Construct paths\n",
    "    tokenizer_path = f\"{model_dir}/tokenizer\"\n",
    "    text_encoder_path = f\"{model_dir}/text_encoder\"\n",
    "    vae_path = f\"{model_dir}/vae\"\n",
    "    unet_path = f\"{model_dir}/unet\"\n",
    "    scheduler_path = f\"{model_dir}/scheduler\"\n",
    "    checkpoint_path = f\"{model_dir}/{checkpoint_name}/pytorch_model.bin\"\n",
    "\n",
    "    # Load components\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(text_encoder_path)\n",
    "    vae = AutoencoderKL.from_pretrained(vae_path)\n",
    "    unet = UNet2DConditionModel.from_pretrained(unet_path)\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(scheduler_path)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CustomStableDiffusionPipeline(\n",
    "        text_encoder=text_encoder,\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        tokenizer=tokenizer,\n",
    "        noise_scheduler=noise_scheduler,\n",
    "        size=512\n",
    "    )\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    if 'text_encoder_state_dict' in checkpoint:\n",
    "        model.text_encoder.load_state_dict(checkpoint['text_encoder_state_dict'])\n",
    "    if 'vae_state_dict' in checkpoint:\n",
    "        model.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
    "    if 'unet_state_dict' in checkpoint:\n",
    "        model.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    model.text_encoder.eval()\n",
    "    model.vae.eval()\n",
    "    model.unet.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def preprocess_image(image_path, size=512, center_crop=False, interpolation=\"bicubic\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Select interpolation mode\n",
    "    interpolation_mode = transforms.InterpolationMode.BICUBIC if interpolation == \"bicubic\" else transforms.InterpolationMode.BILINEAR\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(size) if center_crop else transforms.Resize((size, size), interpolation=interpolation_mode),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "\n",
    "model_directory = \"./model_local_save/model_hurricane3000\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = load_model(model_directory)\n",
    "\n",
    "# Now your model and tokenizer are ready to use for inference\n",
    "\n",
    "# Example inference\n",
    "image_path = \"./finaldata/val_hurricane_pre/hurricane-harvey_00000154_pre_disaster_left.png\"\n",
    "text_prompt = \"Post Disaster Satellite Images of an Hurricane affected area.\"\n",
    "image_tensor = preprocess_image(image_path, size=512)\n",
    "\n",
    "# Specify values for the new parameters\n",
    "num_inference_steps = 50\n",
    "temperature = 1.0\n",
    "guidance_scale = 7.5\n",
    "\n",
    "image_tensor = preprocess_image(\"path_to_image\")\n",
    "with torch.no_grad():\n",
    "    generated_image_tensor = model(\n",
    "        image_tensor.unsqueeze(0),\n",
    "        \"Post Disaster Satellite Images of an Hurricane affected area.\",\n",
    "        num_inference_steps=50,\n",
    "        temperature=1.0,\n",
    "        guidance_scale=7.5\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert the generated tensor to an image\n",
    "generated_image = transforms.ToPILImage()(generated_image_tensor.squeeze(0))  # Remove batch dimension\n",
    "generated_image.save(\"./generated_post_disaster/generated_post_disaster_image.png\")  # Save as PNG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing pre-disaster images\n",
    "input_directory = \"./finaldata/test/pre/\"\n",
    "\n",
    "# Directory to save generated post-disaster images\n",
    "output_directory = \"./generated_post_disaster_images/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# List all pre-disaster image files in the input directory\n",
    "input_image_files = os.listdir(input_directory)\n",
    "\n",
    "# Iterate through the pre-disaster images and generate post-disaster images\n",
    "for input_image_file in input_image_files:\n",
    "    # Construct the full path to the input image\n",
    "    input_image_path = os.path.join(input_directory, input_image_file)\n",
    "\n",
    "    # Preprocess the input image\n",
    "    image_tensor = preprocess_image(input_image_path)\n",
    "\n",
    "    # Specify the text prompt for generating post-disaster images\n",
    "    text_prompt = \"Post Disaster Satellite Images of a Hurricane affected area.\"\n",
    "\n",
    "    # Tokenize the text prompt\n",
    "    input_ids = tokenizer(text_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate the post-disaster image\n",
    "        generated_image_tensor = model(image_tensor.unsqueeze(0), input_ids)  # Add batch dimension\n",
    "\n",
    "    # Convert the generated tensor to an image\n",
    "    generated_image = transforms.ToPILImage()(generated_image_tensor.squeeze(0))  # Remove batch dimension\n",
    "\n",
    "    # Save the generated post-disaster image with the same name as the input image\n",
    "    output_image_path = os.path.join(output_directory, input_image_file)\n",
    "    generated_image.save(output_image_path)\n",
    "\n",
    "    print(f\"Generated: {output_image_path}\")\n",
    "\n",
    "print(\"Inference on multiple pre-disaster images completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: ./generated_post_disaster_images/hurricane-florence_00000012_pre_disaster_left.png\n",
      "Generated: ./generated_post_disaster_images/hurricane-michael_00000083_pre_disaster_left.png\n",
      "Generated: ./generated_post_disaster_images/hurricane-florence_00000191_pre_disaster_left.png\n",
      "Generated: ./generated_post_disaster_images/hurricane-harvey_00000210_pre_disaster_left.png\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './generated_post_disaster_images/hurricane-matthew_00000093_pre_disaster_left.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/myid/rk42218/train_model.ipynb Cell 60\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m         \u001b[39m# Save the generated post-disaster image with the same name as the input image\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m         output_image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_directory, input_image_file)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m         generated_image\u001b[39m.\u001b[39;49msave(output_image_path)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerated: \u001b[39m\u001b[39m{\u001b[39;00moutput_image_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcscuda.cs.uga.edu/home/myid/rk42218/train_model.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInference on multiple pre-disaster images completed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/img2img/lib/python3.8/site-packages/PIL/Image.py:2410\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2408\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2409\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2410\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mw+b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2412\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2413\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './generated_post_disaster_images/hurricane-matthew_00000093_pre_disaster_left.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from edited_code import CustomStableDiffusionPipeline\n",
    "\n",
    "def load_model(model_dir, checkpoint_name=\"checkpoint-3000\"):\n",
    "    # Construct paths\n",
    "    tokenizer_path = f\"{model_dir}/tokenizer\"\n",
    "    text_encoder_path = f\"{model_dir}/text_encoder\"\n",
    "    vae_path = f\"{model_dir}/vae\"\n",
    "    unet_path = f\"{model_dir}/unet\"\n",
    "    scheduler_path = f\"{model_dir}/scheduler\"\n",
    "    checkpoint_path = f\"{model_dir}/{checkpoint_name}/pytorch_model.bin\"\n",
    "\n",
    "    # Load components\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(text_encoder_path)\n",
    "    vae = AutoencoderKL.from_pretrained(vae_path)\n",
    "    unet = UNet2DConditionModel.from_pretrained(unet_path)\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(scheduler_path)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CustomStableDiffusionPipeline(\n",
    "        text_encoder=text_encoder,\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        tokenizer=tokenizer,\n",
    "        noise_scheduler=noise_scheduler,\n",
    "        size=512\n",
    "    )\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    if 'text_encoder_state_dict' in checkpoint:\n",
    "        model.text_encoder.load_state_dict(checkpoint['text_encoder_state_dict'])\n",
    "    if 'vae_state_dict' in checkpoint:\n",
    "        model.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
    "    if 'unet_state_dict' in checkpoint:\n",
    "        model.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    model.text_encoder.eval()\n",
    "    model.vae.eval()\n",
    "    model.unet.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def preprocess_image(image_path, size=512, center_crop=False, interpolation=\"bicubic\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Select interpolation mode\n",
    "    interpolation_mode = transforms.InterpolationMode.BICUBIC if interpolation == \"bicubic\" else transforms.InterpolationMode.BILINEAR\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(size) if center_crop else transforms.Resize((size, size), interpolation=interpolation_mode),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "#import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Directory containing pre-disaster images\n",
    "input_directory = \"./finaldata/val_hurricane_pre/\"\n",
    "\n",
    "# Directory to save generated post-disaster images\n",
    "output_directory = \"./generated_post_disaster_images/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through the pre-disaster images and generate post-disaster images\n",
    "for input_image_file in os.listdir(input_directory):\n",
    "    # Check for image file extensions\n",
    "    if input_image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        # Construct the full path to the input image\n",
    "        input_image_path = os.path.join(input_directory, input_image_file)\n",
    "\n",
    "        # Preprocess the input image\n",
    "        image_tensor = preprocess_image(input_image_path)\n",
    "\n",
    "        # Specify the text prompt for generating post-disaster images\n",
    "        text_prompt = \"Post Disaster Satellite Images of a Hurricane affected area.\"\n",
    "\n",
    "        # Tokenize the text prompt\n",
    "        input_ids = tokenizer(text_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate the post-disaster image\n",
    "            generated_image_tensor = model(image_tensor.unsqueeze(0), input_ids)  # Add batch dimension\n",
    "\n",
    "        # Convert the generated tensor to an image\n",
    "        generated_image = transforms.ToPILImage()(generated_image_tensor.squeeze(0))  # Remove batch dimension\n",
    "\n",
    "        # Save the generated post-disaster image with the same name as the input image\n",
    "        output_image_path = os.path.join(output_directory, input_image_file)\n",
    "        generated_image.save(output_image_path)\n",
    "\n",
    "        print(f\"Generated: {output_image_path}\")\n",
    "\n",
    "print(\"Inference on multiple pre-disaster images completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export PRE_DATA_DIR=\"./finaldata/hurricane_pre/\"\n",
    "export POST_DATA_DIR=\"./finaldata/hurricane_post/\"\n",
    "export PRE_CSV_FILE=\"./finaldata/pre_hurricane_prompts.csv\"\n",
    "export POST_CSV_FILE=\"./finaldata/post_hurricane_prompts.csv\"\n",
    "export CUDA_VISIBLE_DEVICES=1,2,3\n",
    "\n",
    "accelerate launch --num_processes=3 ./edited_code.py \\\n",
    "  --pre_disaster_data_dir=$PRE_DATA_DIR \\\n",
    "  --post_disaster_data_dir=$POST_DATA_DIR \\\n",
    "  --pre_disaster_csv_file=$PRE_CSV_FILE \\\n",
    "  --post_disaster_csv_file=$POST_CSV_FILE \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --placeholder_token=\"<disaster>\" \\\n",
    "  --initializer_token=\"disaster\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=3 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3000 \\\n",
    "  --learning_rate=2.0e-04 \\\n",
    "  --output_dir=\"./model_local_save/model_hurricane3000_demo/\" \\\n",
    "  --save_as_full_pipeline \\\n",
    "  --report_to=wandb \\\n",
    "  --validation_prompt \"Post Disaster Satellite Images of an Hurricane effected area.\" \\\n",
    "  --num_validation_images 4 \\\n",
    "  --validation_steps 3000 \\\n",
    "  --seed 42 \\\n",
    "  --checkpointing_steps 500 \\\n",
    "  --logging_dir \"./logs\" \\\n",
    "  --dataloader_num_workers 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img2img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
